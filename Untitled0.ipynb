{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzJOpKIQAVDRGwEVhbqsOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsham123/GitHub-Lab009/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEXGZhleZ8UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a8f2f9-673e-4c8d-8e8d-6d1b2a89f38f"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def Web_scraping(root):\n",
        "\n",
        "    website = f'{root}/movies'\n",
        "    result = requests.get(website)\n",
        "    content = result.text\n",
        "    soup = BeautifulSoup(content, 'lxml')\n",
        "\n",
        "    box = soup.find('article', class_='main-article')\n",
        "\n",
        "    links = [link['href'] for link in box.find_all('a', href=True)]\n",
        "\n",
        "    for link in links:\n",
        "        result = requests.get(f'{root}/{link}')\n",
        "        content = result.text\n",
        "        soup = BeautifulSoup(content, 'lxml')\n",
        "\n",
        "        box = soup.find('article', class_='main-article')\n",
        "        title = box.find('h1').get_text()\n",
        "        transcript = box.find(\n",
        "            'div', class_='full-script').get_text(strip=True, separator=' ')\n",
        "        i = 0\n",
        "        while i < 10:\n",
        "            with open('Script.txt', 'a', encoding=\"utf-8\") as file:\n",
        "                file.write(transcript)\n",
        "            i = i + 1\n",
        "\n",
        "#Web_scraping('https://subslikescript.com')\n",
        "\n",
        "\n",
        "f = open('Script.txt','r',encoding = 'utf-8') \n",
        "line = f.readline()\n",
        "#print(line)\n",
        "#x = line.split(' ')\n",
        "#print(len(x))\n",
        "\n",
        "\n",
        "def cleanText(document):\n",
        "    tokens = document.split()\n",
        "    table = str.maketrans(' ',' ',string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "tokens = cleanText(line)\n",
        "#print(len(set(tokens)))\n",
        "\n",
        "length = 18 + 1\n",
        "\n",
        "lines = []\n",
        "for i in range(51,len(tokens)):\n",
        "    seq = tokens[i - length : i]\n",
        "    l = ' '.join(seq)\n",
        "    lines.append(l)\n",
        "    if i > 120000:\n",
        "        break\n",
        "#print(lines[0])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "x,y = sequences[:, :-1],sequences[:,-1]\n",
        "y[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "273"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0"
      ],
      "metadata": {
        "id": "5bTccTC0suvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "8ymZt9F8sysh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbTKbbw7aE8Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}